{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gym\n",
    "from gym import spaces\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "import optuna\n",
    "from optuna.visualization import plot_optimization_history, plot_param_importances\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_study(study, filename):\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(study, f)\n",
    "\n",
    "def load_study(filename):\n",
    "    if os.path.isfile(filename):\n",
    "        with open(filename, 'rb') as f:\n",
    "            study = pickle.load(f)\n",
    "        return study\n",
    "    return None\n",
    "\n",
    "# Create or load the study\n",
    "study_filename = 'ppo_study.pkl'\n",
    "study = load_study(study_filename)\n",
    "if study is None:\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "\n",
    "import torch\n",
    "\n",
    "def save_checkpoint(agent, optimizer_actor, optimizer_critic, episode, filename):\n",
    "    checkpoint = {\n",
    "        'agent_state_dict': agent.state_dict(),\n",
    "        'optimizer_actor_state_dict': optimizer_actor.state_dict(),\n",
    "        'optimizer_critic_state_dict': optimizer_critic.state_dict(),\n",
    "        'episode': episode\n",
    "    }\n",
    "    torch.save(checkpoint, filename)\n",
    "\n",
    "def load_checkpoint(agent, optimizer_actor, optimizer_critic, filename):\n",
    "    if os.path.isfile(filename):\n",
    "        checkpoint = torch.load(filename)\n",
    "        agent.load_state_dict(checkpoint['agent_state_dict'])\n",
    "        optimizer_actor.load_state_dict(checkpoint['optimizer_actor_state_dict'])\n",
    "        optimizer_critic.load_state_dict(checkpoint['optimizer_critic_state_dict'])\n",
    "        episode = checkpoint['episode']\n",
    "        return episode\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the preprocessed data\n",
    "file_path = 'input.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Define headers\n",
    "stock_headers = ['INFY', 'BSOFT', 'BBOX', 'ACCELYA', 'HBLPOWER', 'BOSCHLTD', 'NCC', 'AUROPHARMA', 'NATCOPHARM', 'SHRIRAMFIN', 'HINDUNILVR', 'SBIN', 'DRREDDY', 'BHARTIARTL', 'ONGC']\n",
    "bond_headers = ['IN5Y', 'IN10Y']\n",
    "macro_headers = ['Inflation', 'GDP', 'Unemployment', 'Repo Rate', 'Corporate Tax rate', 'IIP', 'Exchange Rate']\n",
    "tech_indicators = ['SMA_20', 'EMA_20', 'EMA_50', 'RSI', 'BB_High', 'BB_Low', 'BB_Mid', 'MACD', 'MACD_Signal',\n",
    "                   'MACD_Diff', 'ATR', 'Stoch', 'Stoch_Signal', \n",
    "                   'SMA_20_x_RSI', 'SMA_20_x_MACD', 'RSI_x_MACD']\n",
    "\n",
    "# Combine all headers\n",
    "features = stock_headers + bond_headers + macro_headers + tech_indicators\n",
    "\n",
    "# Split the data into training (90%) and testing (10%) sets\n",
    "split_idx = int(len(data) * 0.9)\n",
    "train_data = data.iloc[:split_idx]\n",
    "test_data = data.iloc[split_idx:]\n",
    "\n",
    "# Handle missing values and infinities\n",
    "train_data[features] = train_data[features].replace([np.inf, -np.inf], np.nan)\n",
    "train_data[features] = train_data[features].ffill().bfill()\n",
    "\n",
    "# State data including stocks, bonds, and macroeconomic indicators for training\n",
    "state_data = train_data[features].values\n",
    "\n",
    "# Calculate daily returns\n",
    "def calculate_daily_returns(prices):\n",
    "    returns = prices.pct_change().dropna()\n",
    "    return returns\n",
    "\n",
    "daily_returns = train_data[stock_headers + bond_headers].apply(calculate_daily_returns)\n",
    "\n",
    "def ewma_volatility(returns, lambda_=0.94):\n",
    "    squared_returns = returns ** 2\n",
    "    ewma_variance = squared_returns.ewm(span=(2 / (1 - lambda_)) - 1, adjust=False).mean()\n",
    "    ewma_volatility = np.sqrt(ewma_variance)\n",
    "    return ewma_volatility\n",
    "\n",
    "volatilities = daily_returns.apply(ewma_volatility).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, max_size=1000000):\n",
    "        self.buffer = []\n",
    "        self.max_size = max_size\n",
    "        self.ptr = 0\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        if len(self.buffer) < self.max_size:\n",
    "            self.buffer.append((state, action, reward, next_state, done))\n",
    "        else:\n",
    "            self.buffer[self.ptr] = (state, action, reward, next_state, done)\n",
    "        self.ptr = (self.ptr + 1) % self.max_size\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        idx = np.random.choice(len(self.buffer), batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*[self.buffer[i] for i in idx])\n",
    "        return np.array(states), np.array(actions), np.array(rewards), np.array(next_states), np.array(dones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PortfolioEnv(gym.Env):\n",
    "    def __init__(self, state_data, volatilities, stock_indices):\n",
    "        super(PortfolioEnv, self).__init__()\n",
    "        self.state_data = state_data\n",
    "        self.volatilities = volatilities.values\n",
    "        self.stock_indices = stock_indices\n",
    "        self.n_steps, self.n_features = state_data.shape\n",
    "        self.action_space = spaces.Box(low=0, high=1, shape=(len(stock_indices),), dtype=np.float32)\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(self.n_features,), dtype=np.float32)\n",
    "        self.current_step = 0\n",
    "        self.done = False\n",
    "        self.risk_free_rate = 0.03  # Risk-free rate\n",
    "        self.reward_history = []  # To keep track of reward history for normalization\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.done = False\n",
    "        return self.state_data[self.current_step]\n",
    "\n",
    "    def step(self, action_weights):\n",
    "        if self.current_step >= self.n_steps - 2:  # Ensure next step is within bounds\n",
    "            self.done = True\n",
    "            return self.state_data[self.current_step], 0, self.done, {}\n",
    "\n",
    "        self.current_step += 1\n",
    "        reward = self.calculate_reward(action_weights)\n",
    "        state = self.state_data[self.current_step]\n",
    "        return state, reward, self.done, {}\n",
    "\n",
    "    def calculate_reward(self, action_weights):\n",
    "        if self.current_step >= self.n_steps - 1:  # Ensure next step is within bounds\n",
    "            return 0\n",
    "\n",
    "        # Calculate returns using only the stock indices\n",
    "        returns = self.state_data[self.current_step + 1, self.stock_indices] - self.state_data[self.current_step, self.stock_indices]\n",
    "        portfolio_return = np.sum(action_weights * returns)\n",
    "        portfolio_volatility = np.sqrt(np.sum(action_weights ** 2 * self.volatilities[self.current_step][self.stock_indices]))\n",
    "\n",
    "        sharpe_ratio = (portfolio_return - self.risk_free_rate) / (portfolio_volatility + 1e-10)\n",
    "        reward = sharpe_ratio - 0.01 * portfolio_volatility  # Added volatility penalty\n",
    "        \n",
    "        self.reward_history.append(reward)\n",
    "        \n",
    "        if len(self.reward_history) > 1:\n",
    "            mean_reward = np.mean(self.reward_history)\n",
    "            std_reward = np.std(self.reward_history)\n",
    "            reward = (reward - mean_reward) / (std_reward + 1e-10)\n",
    "\n",
    "        return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, input_dim, num_heads, output_dim):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.output_dim = output_dim\n",
    "        self.head_dim = output_dim // num_heads  # Each head's dimension\n",
    "        assert self.head_dim * num_heads == self.output_dim, \"output_dim must be divisible by num_heads\"\n",
    "        self.attention_heads = nn.ModuleList([nn.Linear(input_dim, self.head_dim) for _ in range(num_heads)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = [head(x) for head in self.attention_heads]\n",
    "        outputs = torch.cat(outputs, dim=-1)  # Concatenate along the last dimension\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOAgent(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, action_space, hidden_dim=256, num_heads=4, output_dim=128, lr=0.0003, eps_clip=0.2, gamma=0.99, entropy_weight=0.01):\n",
    "        super(PPOAgent, self).__init__()\n",
    "        self.attention = MultiHeadAttention(state_dim, num_heads, output_dim)\n",
    "        self.actor = self.build_network(output_dim, action_dim * 2, hidden_dim)  # Mean and log_std for normal distribution\n",
    "        self.critic = self.build_network(output_dim, 1, hidden_dim)\n",
    "        \n",
    "        self.optimizer_actor = optim.Adam(self.actor.parameters(), lr=lr)\n",
    "        self.optimizer_critic = optim.Adam(self.critic.parameters(), lr=lr)\n",
    "        \n",
    "        self.scheduler_actor = optim.lr_scheduler.StepLR(self.optimizer_actor, step_size=100, gamma=0.9)\n",
    "        self.scheduler_critic = optim.lr_scheduler.StepLR(self.optimizer_critic, step_size=100, gamma=0.9)\n",
    "        \n",
    "        self.eps_clip = eps_clip\n",
    "        self.gamma = gamma\n",
    "        self.entropy_weight = entropy_weight\n",
    "        self.entropy_decay_rate = 0.995\n",
    "        self.action_space = action_space\n",
    "\n",
    "    def build_network(self, input_dim, output_dim, hidden_dim):\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim),\n",
    "            nn.Softmax(dim=-1) if output_dim > 1 else nn.Identity()\n",
    "        )\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state = torch.FloatTensor(state).unsqueeze(0)\n",
    "        attention_output = self.attention(state)\n",
    "        mean, log_std = self.actor(attention_output).chunk(2, dim=-1)\n",
    "        log_std = torch.clamp(log_std, -20, 2)\n",
    "        std = log_std.exp()\n",
    "        normal = Normal(mean, std)\n",
    "        action = normal.sample()\n",
    "        action = torch.tanh(action)\n",
    "        return action.squeeze(0).cpu().detach().numpy(), normal.log_prob(action).sum(-1), normal.entropy().sum(-1)\n",
    "\n",
    "    def compute_returns(self, rewards, dones):\n",
    "        returns = []\n",
    "        R = 0\n",
    "        for r, d in zip(reversed(rewards), reversed(dones)):\n",
    "            R = r + self.gamma * R * (1 - d)\n",
    "            returns.insert(0, R)\n",
    "        return torch.FloatTensor(returns)\n",
    "\n",
    "    def update(self, states, actions, log_probs, returns, entropies):\n",
    "        states = torch.FloatTensor(np.array(states))\n",
    "        actions = torch.FloatTensor(np.array(actions))\n",
    "        log_probs = torch.stack(log_probs).detach()\n",
    "        returns = torch.FloatTensor(returns)\n",
    "        entropies = torch.stack(entropies).detach()\n",
    "\n",
    "        attention_output = self.attention(states)\n",
    "        advantages = returns - self.critic(attention_output).squeeze().detach()\n",
    "\n",
    "        for _ in range(10):\n",
    "            mean, log_std = self.actor(attention_output).chunk(2, dim=-1)\n",
    "            std = log_std.exp()\n",
    "            normal = Normal(mean, std)\n",
    "            new_log_probs = normal.log_prob(actions).sum(-1)\n",
    "            ratio = torch.exp(new_log_probs - log_probs)\n",
    "            surr1 = ratio * advantages\n",
    "            surr2 = torch.clamp(ratio, 1 - self.eps_clip, 1 + self.eps_clip) * advantages\n",
    "            actor_loss = -torch.min(surr1, surr2).mean()\n",
    "\n",
    "            critic_loss = advantages.pow(2).mean()\n",
    "            entropy_loss = -entropies.mean() * self.entropy_weight\n",
    "\n",
    "            # Add L2 regularization\n",
    "            l2_reg = torch.tensor(0.)\n",
    "            for param in self.actor.parameters():\n",
    "                l2_reg += torch.norm(param)\n",
    "            loss = actor_loss + 0.5 * critic_loss + entropy_loss + l2_reg\n",
    "\n",
    "            self.optimizer_actor.zero_grad()\n",
    "            self.optimizer_critic.zero_grad()\n",
    "            loss.backward(retain_graph=True)\n",
    "            self.optimizer_actor.step()\n",
    "            self.optimizer_critic.step()\n",
    "\n",
    "            # Logging for debugging\n",
    "            print(f\"Update Step, Actor Loss: {actor_loss.item()}, Critic Loss: {critic_loss.item()}, Entropy Loss: {entropy_loss.item()}, Total Loss: {loss.item()}\")\n",
    "\n",
    "        # Step the learning rate schedulers\n",
    "        self.scheduler_actor.step()\n",
    "        self.scheduler_critic.step()\n",
    "\n",
    "        # Clear cache after each update\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    def decay_entropy_weight(self):\n",
    "        self.entropy_weight *= self.entropy_decay_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup TensorBoard and Early Stopping\n",
    "writer = SummaryWriter()\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=100, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.best_score = None\n",
    "        self.counter = 0\n",
    "\n",
    "    def step(self, score):\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            return False\n",
    "        elif score < self.best_score + self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.counter = 0\n",
    "        return False\n",
    "\n",
    "early_stopping = None  # EarlyStopping(patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ppo(env, agent, episodes=1000, writer=None, save_interval=10, checkpoint_path=\"ppo_checkpoint.pth\"):\n",
    "    start_episode = load_checkpoint(agent, agent.optimizer_actor, agent.optimizer_critic, checkpoint_path)\n",
    "    scheduler = optim.lr_scheduler.StepLR(agent.optimizer_actor, step_size=3, gamma=0.9)  # Learning rate scheduler\n",
    "    for ep in range(start_episode, episodes):\n",
    "        state = env.reset()\n",
    "        states, actions, log_probs, rewards, dones, entropies = [], [], [], [], [], []\n",
    "        done = False\n",
    "        while not done:\n",
    "            action, log_prob, entropy = agent.select_action(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            states.append(state)\n",
    "            actions.append(action)\n",
    "            log_probs.append(log_prob)\n",
    "            rewards.append(reward)\n",
    "            dones.append(done)\n",
    "            entropies.append(entropy)\n",
    "            state = next_state\n",
    "\n",
    "        returns = agent.compute_returns(rewards, dones)\n",
    "        agent.update(states, actions, log_probs, returns, entropies)\n",
    "\n",
    "        episode_return = sum(rewards)\n",
    "        print(f\"Episode {ep}, Return: {episode_return}\")\n",
    "\n",
    "        if writer:\n",
    "            writer.add_scalar(\"Return\", episode_return, ep)\n",
    "\n",
    "        # Save checkpoint\n",
    "        if ep % save_interval == 0:\n",
    "            save_checkpoint(agent, agent.optimizer_actor, agent.optimizer_critic, ep, checkpoint_path)\n",
    "\n",
    "        # Update learning rate and decay entropy weight\n",
    "        scheduler.step()\n",
    "        agent.decay_entropy_weight()\n",
    "\n",
    "    print(f\"Final Weights (PPO): {agent.latest_action}\")\n",
    "    print(f\"Training is complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_agent(agent, filename):\n",
    "    torch.save(agent.state_dict(), filename)\n",
    "\n",
    "def load_agent(agent, filename):\n",
    "    if os.path.isfile(filename):\n",
    "        checkpoint = torch.load(filename)\n",
    "        agent.load_state_dict(checkpoint, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import torch\n",
    "\n",
    "study_filename = 'ppo_study.pkl'\n",
    "checkpoint_path = 'ppo_checkpoint.pth'\n",
    "\n",
    "# Create or load the study\n",
    "study = load_study(study_filename)\n",
    "if study is None:\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "\n",
    "def objective(trial):\n",
    "    # Define hyperparameters\n",
    "    hidden_dim = trial.suggest_int('hidden_dim', 64, 256)\n",
    "    lr = trial.suggest_float('lr', 1e-5, 1e-3, log=True)\n",
    "    eps_clip = trial.suggest_float('eps_clip', 0.1, 0.3)\n",
    "    gamma = trial.suggest_float('gamma', 0.9, 0.99)\n",
    "    entropy_weight = trial.suggest_float('entropy_weight', 1e-3, 1e-2, log=True)\n",
    "    num_heads = trial.suggest_int('num_heads', 2, 8)\n",
    "    output_dim_base = trial.suggest_int('output_dim_base', 16, 64)\n",
    "\n",
    "    output_dim = output_dim_base * num_heads\n",
    "\n",
    "    agent = PPOAgent(state_dim, action_dim, env.action_space, hidden_dim, num_heads, output_dim, lr, eps_clip, gamma, entropy_weight)\n",
    "\n",
    "    # Train the agent\n",
    "    def train_ppo_trial(env, agent, episodes=30):\n",
    "        total_rewards = []\n",
    "        for ep in range(episodes):\n",
    "            state = env.reset()\n",
    "            states, actions, log_probs, rewards, dones, entropies = [], [], [], [], [], []\n",
    "            done = False\n",
    "            step = 0\n",
    "            while not done:\n",
    "                step += 1\n",
    "                action, log_prob, entropy = agent.select_action(state)\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "                states.append(state)\n",
    "                actions.append(action)\n",
    "                log_probs.append(log_prob)\n",
    "                rewards.append(reward)\n",
    "                dones.append(done)\n",
    "                entropies.append(entropy)\n",
    "                state = next_state\n",
    "\n",
    "                if step % 100 == 0:\n",
    "                    print(f\"Episode {ep}, Step {step}, Reward: {reward}\")\n",
    "\n",
    "            returns = agent.compute_returns(rewards, dones)\n",
    "            agent.update(states, actions, log_probs, returns, entropies)\n",
    "\n",
    "            episode_return = sum(rewards)\n",
    "            total_rewards.append(episode_return)\n",
    "            trial.report(episode_return, ep)\n",
    "\n",
    "            if trial.should_prune():\n",
    "                raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "        return sum(total_rewards)\n",
    "\n",
    "    total_rewards = train_ppo_trial(env, agent)\n",
    "    return float(total_rewards)\n",
    "\n",
    "# Optimize the study\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "# Save the study\n",
    "save_study(study, study_filename)\n",
    "\n",
    "# Best hyperparameters\n",
    "best_params = study.best_params\n",
    "print(\"Best hyperparameters found were: \", best_params)\n",
    "\n",
    "# Initialize the PPO agent with the best hyperparameters\n",
    "ppo_agent = PPOAgent(\n",
    "    state_dim,\n",
    "    action_dim,\n",
    "    env.action_space,\n",
    "    hidden_dim=best_params['hidden_dim'],\n",
    "    num_heads=best_params['num_heads'],\n",
    "    output_dim=best_params['output_dim_base'] * best_params['num_heads'],\n",
    "    lr=best_params['lr'],\n",
    "    eps_clip=best_params['eps_clip'],\n",
    "    gamma=best_params['gamma'],\n",
    "    entropy_weight=best_params['entropy_weight'],\n",
    "    regularization_weight=0.0001  # Light regularization\n",
    ")\n",
    "\n",
    "# Train PPO agent\n",
    "num_episodes = 500  # Define the number of episodes you want to train for\n",
    "train_ppo(env, ppo_agent, episodes=num_episodes, writer=writer, save_interval=10, checkpoint_path=checkpoint_path)\n",
    "\n",
    "# Save trained PPO model\n",
    "save_agent(ppo_agent, 'ppo_agent_final.pth')\n",
    "print(\"Training complete. PPO model saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cvxpy as cp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the CSV file for MVO\n",
    "mvo_file_path = 'MVO.csv'\n",
    "mvo_data = pd.read_csv(mvo_file_path)\n",
    "\n",
    "# Define stock headers (assuming they are the first 15 columns)\n",
    "stock_headers = mvo_data.columns[1:16]  # Exclude the Date column\n",
    "\n",
    "# Calculate daily returns for MVO\n",
    "daily_returns_mvo = mvo_data[stock_headers].pct_change().dropna()\n",
    "\n",
    "# Calculate expected returns and covariance matrix for MVO using raw stock data\n",
    "expected_returns_mvo = daily_returns_mvo.mean().values\n",
    "cov_matrix_mvo = daily_returns_mvo.cov().values\n",
    "\n",
    "# Mean-Variance Optimization (MVO) function\n",
    "def mean_variance_optimization(expected_returns, cov_matrix, risk_aversion=1):\n",
    "    n = len(expected_returns)\n",
    "    w = cp.Variable(n)\n",
    "    objective = cp.Maximize(expected_returns.T @ w - risk_aversion * cp.quad_form(w, cov_matrix))\n",
    "    constraints = [cp.sum(w) == 1, w >= 0]\n",
    "    problem = cp.Problem(objective, constraints)\n",
    "    problem.solve()\n",
    "    \n",
    "    # Ensure the weights sum to 1\n",
    "    weights = w.value\n",
    "    weights /= np.sum(weights)\n",
    "    \n",
    "    return weights\n",
    "\n",
    "# Calculate MVO weights\n",
    "mvo_weights = mean_variance_optimization(expected_returns_mvo, cov_matrix_mvo)\n",
    "print(\"MVO Weights:\", mvo_weights)\n",
    "print(\"Sum of MVO Weights:\", np.sum(mvo_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to combine PPO weights with MVO weights\n",
    "def combine_weights(ppo_weights, mvo_weights, alpha):\n",
    "    combined_weights = (1 - alpha) * ppo_weights + alpha * mvo_weights\n",
    "    return combined_weights\n",
    "\n",
    "# Function to sweep alpha and evaluate combined weights\n",
    "def evaluate_combined_weights(env, ppo_agent, alpha_values):\n",
    "    results = []\n",
    "    for alpha in alpha_values:\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        episode_return = 0\n",
    "        while not done:\n",
    "            ppo_weights, _, _ = ppo_agent.select_action(state)\n",
    "            # Only take the stock weights from the PPO agent\n",
    "            ppo_stock_weights = ppo_weights[:len(stock_headers)]\n",
    "            combined_weights = combine_weights(ppo_stock_weights, mvo_weights, alpha)\n",
    "            # Ensure the action weights include only the stock weights for the environment step\n",
    "            next_state, reward, done, _ = env.step(combined_weights[:len(stock_headers)])\n",
    "            episode_return += reward\n",
    "            state = next_state\n",
    "        results.append((alpha, episode_return))\n",
    "        print(f\"Alpha: {alpha}, Episode Return: {episode_return}\")\n",
    "    return results\n",
    "\n",
    "# Alpha values to sweep from 0 to 1\n",
    "alpha_values = np.linspace(0, 1, 11)\n",
    "\n",
    "# Evaluate combined weights\n",
    "results = evaluate_combined_weights(env, ppo_agent, alpha_values)\n",
    "\n",
    "# Save results to visualize later\n",
    "results_df = pd.DataFrame(results, columns=['Alpha', 'Episode Return'])\n",
    "results_df.to_csv('combined_weights_results.csv', index=False)\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(results_df['Alpha'], results_df['Episode Return'], marker='o')\n",
    "plt.title('Performance of Combined Weights for Different Alpha Values')\n",
    "plt.xlabel('Alpha')\n",
    "plt.ylabel('Episode Return')\n",
    "plt.grid(True)\n",
    "plt.savefig('combined_weights_performance.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model evaluation\n",
    "def calculate_performance_metrics(returns):\n",
    "    metrics = {}\n",
    "    metrics['ROI'] = np.sum(returns)\n",
    "    metrics['Sharpe Ratio'] = np.mean(returns) / (np.std(returns) + 1e-10)\n",
    "    downside_returns = returns[returns < 0]\n",
    "    metrics['Sortino Ratio'] = np.mean(returns) / (np.std(downside_returns) + 1e-10)\n",
    "    cum_returns = np.cumsum(returns)\n",
    "    drawdown = np.max(cum_returns) - cum_returns\n",
    "    metrics['Maximum Drawdown'] = np.max(drawdown)\n",
    "    metrics['Calmar Ratio'] = metrics['ROI'] / (metrics['Maximum Drawdown'] + 1e-10)\n",
    "    return metrics\n",
    "\n",
    "# Backtesting\n",
    "def backtest(env, agent, alpha_values):\n",
    "    backtest_results = {}\n",
    "    for alpha in alpha_values:\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        returns = []\n",
    "        while not done:\n",
    "            ppo_weights, _, _ = agent.select_action(state)\n",
    "            ppo_stock_weights = ppo_weights[:len(stock_headers)]\n",
    "            combined_weights = combine_weights(ppo_stock_weights, mvo_weights, alpha)\n",
    "            next_state, reward, done, _ = env.step(combined_weights)\n",
    "            returns.append(reward)\n",
    "            state = next_state\n",
    "        metrics = calculate_performance_metrics(np.array(returns))\n",
    "        backtest_results[alpha] = metrics\n",
    "        print(f\"Alpha: {alpha}, Metrics: {metrics}\")\n",
    "    return backtest_results\n",
    "\n",
    "# Perform backtesting\n",
    "backtest_results = backtest(env, ppo_agent, alpha_values)\n",
    "\n",
    "# Save backtest results\n",
    "backtest_df = pd.DataFrame(backtest_results).T\n",
    "backtest_df.to_csv('backtest_results.csv', index=False)\n",
    "\n",
    "# Plot backtest results\n",
    "backtest_df.plot(kind='bar', figsize=(12, 8))\n",
    "plt.title('Backtest Performance Metrics for Different Alpha Values')\n",
    "plt.xlabel('Alpha')\n",
    "plt.ylabel('Metrics')\n",
    "plt.legend(loc='best')\n",
    "plt.savefig('backtest_performance_metrics.png')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
